{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install jax jaxlib flax transformers datasets","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:05:50.738030Z","iopub.execute_input":"2024-06-10T14:05:50.738362Z","iopub.status.idle":"2024-06-10T14:06:05.081188Z","shell.execute_reply.started":"2024-06-10T14:05:50.738322Z","shell.execute_reply":"2024-06-10T14:06:05.080221Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (0.4.26)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (0.4.26.dev20240504)\nRequirement already satisfied: flax in /opt/conda/lib/python3.10/site-packages (0.8.4)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax) (0.2.0)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax) (1.26.4)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax) (1.11.4)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from flax) (1.0.7)\nRequirement already satisfied: optax in /opt/conda/lib/python3.10/site-packages (from flax) (0.2.2)\nRequirement already satisfied: orbax-checkpoint in /opt/conda/lib/python3.10/site-packages (from flax) (0.5.15)\nRequirement already satisfied: tensorstore in /opt/conda/lib/python3.10/site-packages (from flax) (0.1.60)\nRequirement already satisfied: rich>=11.1 in /opt/conda/lib/python3.10/site-packages (from flax) (13.7.0)\nRequirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.10/site-packages (from flax) (4.9.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from flax) (6.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1->flax) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1->flax) (2.17.2)\nRequirement already satisfied: absl-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from optax->flax) (1.4.0)\nRequirement already satisfied: chex>=0.1.86 in /opt/conda/lib/python3.10/site-packages (from optax->flax) (0.1.86)\nRequirement already satisfied: etils[epath,epy] in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.6.0)\nRequirement already satisfied: nest_asyncio in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.5.8)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax) (3.20.3)\nCollecting ml-dtypes>=0.2.0 (from jax)\n  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: toolz>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chex>=0.1.86->optax->flax) (0.12.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.1)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\nDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ml-dtypes\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\ntensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ml-dtypes-0.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.core import FrozenDict\nimport optax\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Load the dataset\ndataset = load_dataset(\"Helsinki-NLP/opus_books\", \"en-hu\")\n\n# Select smaller subsets of the dataset\ntrain_dataset = dataset['train'].select(range(50000))\nval_dataset = dataset['train'].select(range(50000, 60000))\ntest_dataset = dataset['train'].select(range(60000, 70000))\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hu\")\n\n# Preprocess function\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hu'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize the dataset\ntokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\n\n# Convert to numpy arrays for JAX\ndef convert_to_numpy(tokenized_dataset):\n    input_ids = jnp.array(tokenized_dataset[\"input_ids\"])\n    attention_mask = jnp.array(tokenized_dataset[\"attention_mask\"])\n    labels = jnp.array(tokenized_dataset[\"labels\"])\n    return input_ids, attention_mask, labels\n\ntrain_input_ids, train_attention_mask, train_labels = convert_to_numpy(tokenized_train)\nval_input_ids, val_attention_mask, val_labels = convert_to_numpy(tokenized_val)\ntest_input_ids, test_attention_mask, test_labels = convert_to_numpy(tokenized_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:06:05.083141Z","iopub.execute_input":"2024-06-10T14:06:05.083460Z","iopub.status.idle":"2024-06-10T14:09:30.778511Z","shell.execute_reply.started":"2024-06-10T14:06:05.083429Z","shell.execute_reply":"2024-06-10T14:09:30.777396Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efee52887b6e4d7d80721c693843c2d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/23.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e0b6fe72d1b45b7af4253526bb49c7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/137151 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea276c6f761e4d5dbf2059fd2ff6deac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3809f02ef3f04274b23dda9cf9f1ecf6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4131340c039a46c987b8a2c1998b23cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"890feff0f21344bc82fc631cf9af82ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/850k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"184148c825e542de8fd8c2173afbf954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.57M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b3c07ef0aae4b0d9a24683dc8f76572"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb8b9c3f78fb411694b51f0df11ffbbf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e7a01a9f899437f92e9bb4ad7a85c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95cb099dda540efa35f91ce2e0c8c10"}},"metadata":{}}]},{"cell_type":"code","source":"\n\nclass Transformer(nn.Module):\n    vocab_size: int\n    hidden_dim: int = 256\n    num_heads: int = 4\n    num_layers: int = 3\n    max_length: int = 128\n\n    def setup(self):\n        self.embedding = nn.Embed(self.vocab_size, self.hidden_dim)\n        self.encoder_layers = [nn.SelfAttention(num_heads=self.num_heads, qkv_features=self.hidden_dim) for _ in range(self.num_layers)]\n        self.decoder_layers = [nn.SelfAttention(num_heads=self.num_heads, qkv_features=self.hidden_dim) for _ in range(self.num_layers)]\n        self.output_layer = nn.Dense(self.vocab_size)\n\n    def __call__(self, x, y):\n        x_embed = self.embedding(x)\n        y_embed = self.embedding(y)\n\n        for layer in self.encoder_layers:\n            x_embed = layer(x_embed)\n\n        for layer in self.decoder_layers:\n            y_embed = layer(y_embed)\n\n        logits = self.output_layer(y_embed)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:13:51.799453Z","iopub.execute_input":"2024-06-10T14:13:51.800211Z","iopub.status.idle":"2024-06-10T14:13:51.810225Z","shell.execute_reply.started":"2024-06-10T14:13:51.800171Z","shell.execute_reply":"2024-06-10T14:13:51.809116Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = Transformer(vocab_size=tokenizer.vocab_size)\nprint(\"Model Information:\")\nprint(\"Vocabulary Size:\", model.vocab_size)\nprint(\"Hidden Dimension:\", model.hidden_dim)\nprint(\"Number of Heads:\", model.num_heads)\nprint(\"Number of Layers:\", model.num_layers)\nprint(\"Max Length:\", model.max_length)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:13:54.547504Z","iopub.execute_input":"2024-06-10T14:13:54.548438Z","iopub.status.idle":"2024-06-10T14:13:54.554114Z","shell.execute_reply.started":"2024-06-10T14:13:54.548403Z","shell.execute_reply":"2024-06-10T14:13:54.553203Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model Information:\nVocabulary Size: 62522\nHidden Dimension: 256\nNumber of Heads: 4\nNumber of Layers: 3\nMax Length: 128\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Transformer(vocab_size=tokenizer.vocab_size)\nparams = model.init(jax.random.PRNGKey(0), jnp.ones((1, 128), dtype=jnp.int32), jnp.ones((1, 128), dtype=jnp.int32))[\"params\"]\noptimizer = optax.adam(learning_rate=0.0001)\nopt_state = optimizer.init(params)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:14:02.216620Z","iopub.execute_input":"2024-06-10T14:14:02.217558Z","iopub.status.idle":"2024-06-10T14:14:02.407710Z","shell.execute_reply.started":"2024-06-10T14:14:02.217520Z","shell.execute_reply":"2024-06-10T14:14:02.406960Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Cross entropy loss function\ndef cross_entropy_loss(logits, labels):\n    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n    return jnp.mean(loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:14:03.761312Z","iopub.execute_input":"2024-06-10T14:14:03.761998Z","iopub.status.idle":"2024-06-10T14:14:03.766372Z","shell.execute_reply.started":"2024-06-10T14:14:03.761965Z","shell.execute_reply":"2024-06-10T14:14:03.765460Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Training step\n@jax.jit\ndef train_step(params, opt_state, batch):\n    def loss_fn(params):\n        logits = model.apply({'params': params}, batch[0], batch[2])\n        loss = cross_entropy_loss(logits, batch[2])\n        return loss\n\n    grad_fn = jax.value_and_grad(loss_fn)\n    loss, grads = grad_fn(params)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:14:05.440186Z","iopub.execute_input":"2024-06-10T14:14:05.440524Z","iopub.status.idle":"2024-06-10T14:14:05.446834Z","shell.execute_reply.started":"2024-06-10T14:14:05.440497Z","shell.execute_reply":"2024-06-10T14:14:05.445852Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import time\n\n\nstart_time = time.time()\n\nfor epoch in range(50):\n    for _ in range(5000 // 16):\n        batch = (train_input_ids[_:_+16], train_attention_mask[_:_+16], train_labels[_:_+16])\n        params, opt_state, loss = train_step(params, opt_state, batch)\n    print(f'Epoch {epoch+1} - Loss: {loss}')\n    \ntraining_time = time.time() - start_time\nprint(\"Training Time:\", training_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:14:07.149516Z","iopub.execute_input":"2024-06-10T14:14:07.150266Z","iopub.status.idle":"2024-06-10T14:23:33.064858Z","shell.execute_reply.started":"2024-06-10T14:14:07.150232Z","shell.execute_reply":"2024-06-10T14:23:33.063686Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1 - Loss: 2.083096742630005\nEpoch 2 - Loss: 1.9809261560440063\nEpoch 3 - Loss: 1.9450907707214355\nEpoch 4 - Loss: 1.9234602451324463\nEpoch 5 - Loss: 1.910798192024231\nEpoch 6 - Loss: 1.9001022577285767\nEpoch 7 - Loss: 1.884229302406311\nEpoch 8 - Loss: 1.86797297000885\nEpoch 9 - Loss: 1.1999534368515015\nEpoch 10 - Loss: 1.0686688423156738\nEpoch 11 - Loss: 0.985762357711792\nEpoch 12 - Loss: 0.9380471110343933\nEpoch 13 - Loss: 0.9045101404190063\nEpoch 14 - Loss: 0.8698063492774963\nEpoch 15 - Loss: 0.8451318740844727\nEpoch 16 - Loss: 0.8336489200592041\nEpoch 17 - Loss: 0.8203508257865906\nEpoch 18 - Loss: 0.8065019249916077\nEpoch 19 - Loss: 0.7935856580734253\nEpoch 20 - Loss: 0.8446008563041687\nEpoch 21 - Loss: 0.7896855473518372\nEpoch 22 - Loss: 0.7707398533821106\nEpoch 23 - Loss: 0.7530943751335144\nEpoch 24 - Loss: 0.7365652918815613\nEpoch 25 - Loss: 0.716712474822998\nEpoch 26 - Loss: 0.6967530250549316\nEpoch 27 - Loss: 0.6868158578872681\nEpoch 28 - Loss: 0.6815652251243591\nEpoch 29 - Loss: 0.6743967533111572\nEpoch 30 - Loss: 0.6588245630264282\nEpoch 31 - Loss: 0.6449065208435059\nEpoch 32 - Loss: 0.6288838982582092\nEpoch 33 - Loss: 0.6187921762466431\nEpoch 34 - Loss: 0.6082934737205505\nEpoch 35 - Loss: 0.599425196647644\nEpoch 36 - Loss: 0.8483148813247681\nEpoch 37 - Loss: 0.7821424007415771\nEpoch 38 - Loss: 0.732683539390564\nEpoch 39 - Loss: 0.7152795195579529\nEpoch 40 - Loss: 0.6918972730636597\nEpoch 41 - Loss: 0.7107279896736145\nEpoch 42 - Loss: 0.6621928215026855\nEpoch 43 - Loss: 0.6476194858551025\nEpoch 44 - Loss: 0.6361731886863708\nEpoch 45 - Loss: 0.6239661574363708\nEpoch 46 - Loss: 0.637360692024231\nEpoch 47 - Loss: 0.5975547432899475\nEpoch 48 - Loss: 0.5759090185165405\nEpoch 49 - Loss: 0.5611081123352051\nEpoch 50 - Loss: 0.5419854521751404\nTraining Time: 565.9080328941345 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"def translate(params, input_text):\n    # Tokenize the input text\n    inputs = tokenizer(input_text, return_tensors=\"jax\", padding=\"max_length\", truncation=True, max_length=128)\n    input_ids = inputs[\"input_ids\"]\n    attention_mask = inputs[\"attention_mask\"]\n    print(f\"Input IDs: {input_ids}\")\n    print(f\"Attention Mask: {attention_mask}\")\n\n    # Initialize the output sequence with BOS token\n    output_ids = jnp.zeros((1, 128), dtype=jnp.int32)\n    output_ids = output_ids.at[0, 0].set(tokenizer.bos_token_id)\n\n    generated_tokens = [tokenizer.bos_token_id]\n\n    # Auto-regressive generation\n    for i in range(1, 128):\n        logits = model.apply({'params': params}, input_ids, output_ids)\n        next_token = jnp.argmax(logits[:, i-1], axis=-1).item()  # Convert to scalar integer\n        output_ids = output_ids.at[:, i].set(next_token)\n        generated_tokens.append(next_token)\n\n        if next_token == tokenizer.eos_token_id:\n            break\n            \n        print(f\"Step {i}: Logits shape: {logits.shape}\")\n        print(f\"Step {i}: Next Token: {next_token}\")\n        print(f\"Step {i}: Updated Output IDs: {output_ids}\")\n        \n    # Filter out None values from generated_tokens\n    generated_tokens = [token for token in generated_tokens if token is not None]\n    # Decode the output token IDs to text\n    output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n    print(f\"Generated Tokens: {generated_tokens}\")\n    print(f\"Output Text: {output_text}\")\n\n    return output_text\n\n\n# Example usage\ntranslated_text = translate(params, \"Source: Project GutenbergAudiobook available here\")\nprint(f\"Translated Text: {translated_text}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}