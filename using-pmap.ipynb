{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.core import FrozenDict\nimport optax\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Load the dataset\ndataset = load_dataset(\"Helsinki-NLP/opus_books\", \"en-hu\")\n\n# Select smaller subsets of the dataset\ntrain_dataset = dataset['train'].select(range(50000))\nval_dataset = dataset['train'].select(range(50000, 60000))\ntest_dataset = dataset['train'].select(range(60000, 70000))\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:45:37.735018Z","iopub.execute_input":"2024-06-10T14:45:37.735388Z","iopub.status.idle":"2024-06-10T14:45:39.800962Z","shell.execute_reply.started":"2024-06-10T14:45:37.735354Z","shell.execute_reply":"2024-06-10T14:45:39.800138Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Define the model\nclass Transformer(nn.Module):\n    vocab_size: int\n    hidden_dim: int = 256\n    num_heads: int = 4\n    num_layers: int = 3\n    max_length: int = 128\n\n    def setup(self):\n        self.embedding = nn.Embed(self.vocab_size, self.hidden_dim)\n        self.encoder_layers = [nn.SelfAttention(num_heads=self.num_heads, qkv_features=self.hidden_dim) for _ in range(self.num_layers)]\n        self.decoder_layers = [nn.SelfAttention(num_heads=self.num_heads, qkv_features=self.hidden_dim) for _ in range(self.num_layers)]\n        self.output_layer = nn.Dense(self.vocab_size)\n\n    def __call__(self, x, y):\n        x_embed = self.embedding(x)\n        y_embed = self.embedding(y)\n\n        for layer in self.encoder_layers:\n            x_embed = layer(x_embed)\n\n        for layer in self.decoder_layers:\n            y_embed = layer(y_embed)\n\n        logits = self.output_layer(y_embed)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:45:39.802693Z","iopub.execute_input":"2024-06-10T14:45:39.802979Z","iopub.status.idle":"2024-06-10T14:45:39.813333Z","shell.execute_reply.started":"2024-06-10T14:45:39.802954Z","shell.execute_reply":"2024-06-10T14:45:39.812374Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef train_step(params, opt_state, batch):\n    def loss_fn(params):\n        logits = model.apply({'params': params}, batch[0], batch[2])\n        \n        # Convert labels to one-hot encoding\n        labels_one_hot = jax.nn.one_hot(batch[2], num_classes=tokenizer.vocab_size)\n        \n        # Compute softmax cross-entropy loss\n        loss = optax.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n        \n        return jnp.mean(loss)\n\n    grad_fn = jax.value_and_grad(loss_fn)\n    loss, grads = grad_fn(params)\n    updates, opt_state = optimizer.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:45:39.814567Z","iopub.execute_input":"2024-06-10T14:45:39.814907Z","iopub.status.idle":"2024-06-10T14:45:39.831161Z","shell.execute_reply.started":"2024-06-10T14:45:39.814875Z","shell.execute_reply":"2024-06-10T14:45:39.830365Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Define pmap'd training step\n@jax.jit\ndef mapped_train_step(params, opt_state, batch):\n    params, opt_state, loss = jax.pmap(\n        train_step, axis_name='batch')(params, opt_state, batch)\n    loss = jax.pmean(loss, axis_name='batch')\n    return params, opt_state, loss\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:45:39.833144Z","iopub.execute_input":"2024-06-10T14:45:39.833477Z","iopub.status.idle":"2024-06-10T14:45:39.843139Z","shell.execute_reply.started":"2024-06-10T14:45:39.833417Z","shell.execute_reply":"2024-06-10T14:45:39.842273Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Tokenize the dataset\ndef preprocess_function(examples):\n    inputs = [ex['en'] for ex in examples['translation']]\n    targets = [ex['hu'] for ex in examples['translation']]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\ntokenized_test = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"translation\"])\n\n# Convert to numpy arrays for JAX\ndef convert_to_numpy(tokenized_dataset):\n    input_ids = jnp.array(tokenized_dataset[\"input_ids\"])\n    attention_mask = jnp.array(tokenized_dataset[\"attention_mask\"])\n    labels = jnp.array(tokenized_dataset[\"labels\"])\n    return input_ids, attention_mask, labels\n\ntrain_input_ids, train_attention_mask, train_labels = convert_to_numpy(tokenized_train)\nval_input_ids, val_attention_mask, val_labels = convert_to_numpy(tokenized_val)\ntest_input_ids, test_attention_mask, test_labels = convert_to_numpy(tokenized_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:45:39.844217Z","iopub.execute_input":"2024-06-10T14:45:39.844572Z","iopub.status.idle":"2024-06-10T14:48:14.700612Z","shell.execute_reply.started":"2024-06-10T14:45:39.844540Z","shell.execute_reply":"2024-06-10T14:48:14.699498Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\nmodel = Transformer(vocab_size=tokenizer.vocab_size)\nparams = model.init(jax.random.PRNGKey(0), jnp.ones((1, 128), dtype=jnp.int32), jnp.ones((1, 128), dtype=jnp.int32))[\"params\"]\noptimizer = optax.adam(learning_rate=0.0001)\nopt_state = optimizer.init(params)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:48:14.702131Z","iopub.execute_input":"2024-06-10T14:48:14.702413Z","iopub.status.idle":"2024-06-10T14:48:14.900613Z","shell.execute_reply.started":"2024-06-10T14:48:14.702388Z","shell.execute_reply":"2024-06-10T14:48:14.899644Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"print(\"Train Input IDs shape:\", train_input_ids.shape)\nprint(\"Train Attention Mask shape:\", train_attention_mask.shape)\nprint(\"Train Labels shape:\", train_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:48:14.901851Z","iopub.execute_input":"2024-06-10T14:48:14.902115Z","iopub.status.idle":"2024-06-10T14:48:14.907078Z","shell.execute_reply.started":"2024-06-10T14:48:14.902091Z","shell.execute_reply":"2024-06-10T14:48:14.906227Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Train Input IDs shape: (50000, 128)\nTrain Attention Mask shape: (50000, 128)\nTrain Labels shape: (50000, 128)\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(train_data, num_epochs, batch_size, initial_params, optimizer):\n    opt_state = optimizer.init(initial_params)\n    params = initial_params\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for batch in train_data:\n            params, opt_state, loss = train_step(params, opt_state, batch)\n            epoch_loss += loss\n\n        avg_epoch_loss = epoch_loss / len(train_data)\n        print(f\"Epoch {epoch+1}, Loss: {avg_epoch_loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:48:14.908146Z","iopub.execute_input":"2024-06-10T14:48:14.908458Z","iopub.status.idle":"2024-06-10T14:48:14.919866Z","shell.execute_reply.started":"2024-06-10T14:48:14.908410Z","shell.execute_reply":"2024-06-10T14:48:14.918964Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"optimizer = optax.adam(learning_rate=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:48:14.920923Z","iopub.execute_input":"2024-06-10T14:48:14.921203Z","iopub.status.idle":"2024-06-10T14:48:14.935812Z","shell.execute_reply.started":"2024-06-10T14:48:14.921179Z","shell.execute_reply":"2024-06-10T14:48:14.934970Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"import time\n\n# Record the start time\nstart_time = time.time()\n\n# Define the training parameters\nbatch_size = 16\ntrain_data = (train_input_ids, train_attention_mask, train_labels)\ninitial_params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 128), dtype=jnp.int32), jnp.ones((1, 128), dtype=jnp.int32))[\"params\"]\n\n# Train the model\ntrained_params= train_model(train_data, 50, batch_size, initial_params, optimizer)\n\n# Record the end time\nend_time = time.time()\n\n# Calculate the training time\ntraining_time = end_time - start_time\n\n# Display the training time\nprint(\"Training time:\", training_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:48:14.938640Z","iopub.execute_input":"2024-06-10T14:48:14.938970Z","iopub.status.idle":"2024-06-10T14:48:17.822028Z","shell.execute_reply.started":"2024-06-10T14:48:14.938934Z","shell.execute_reply":"2024-06-10T14:48:17.821065Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 10.527602195739746\nEpoch 2, Loss: 6.242406368255615\nEpoch 3, Loss: 2.7567012310028076\nEpoch 4, Loss: 2.0361435413360596\nEpoch 5, Loss: 1.0315613746643066\nEpoch 6, Loss: 1.1941053867340088\nEpoch 7, Loss: 1.0223288536071777\nEpoch 8, Loss: 1.0766180753707886\nEpoch 9, Loss: 0.9996814131736755\nEpoch 10, Loss: 0.8261687159538269\nEpoch 11, Loss: 0.612897515296936\nEpoch 12, Loss: 0.5480751395225525\nEpoch 13, Loss: 0.4410651922225952\nEpoch 14, Loss: 0.5039677619934082\nEpoch 15, Loss: 0.4538334608078003\nEpoch 16, Loss: 0.43136781454086304\nEpoch 17, Loss: 0.4076271951198578\nEpoch 18, Loss: 0.3754669427871704\nEpoch 19, Loss: 0.3384438455104828\nEpoch 20, Loss: 0.3069915771484375\nEpoch 21, Loss: 0.30940699577331543\nEpoch 22, Loss: 0.2740011215209961\nEpoch 23, Loss: 0.2542256712913513\nEpoch 24, Loss: 0.23851990699768066\nEpoch 25, Loss: 0.23250965774059296\nEpoch 26, Loss: 0.23146545886993408\nEpoch 27, Loss: 0.22077354788780212\nEpoch 28, Loss: 0.21609269082546234\nEpoch 29, Loss: 0.21343132853507996\nEpoch 30, Loss: 0.20891469717025757\nEpoch 31, Loss: 0.20397889614105225\nEpoch 32, Loss: 0.20082303881645203\nEpoch 33, Loss: 0.1974397599697113\nEpoch 34, Loss: 0.19386303424835205\nEpoch 35, Loss: 0.19166997075080872\nEpoch 36, Loss: 0.1905701607465744\nEpoch 37, Loss: 0.1903504729270935\nEpoch 38, Loss: 0.19038206338882446\nEpoch 39, Loss: 0.19015958905220032\nEpoch 40, Loss: 0.189861461520195\nEpoch 41, Loss: 0.1896217316389084\nEpoch 42, Loss: 0.18934687972068787\nEpoch 43, Loss: 0.18919166922569275\nEpoch 44, Loss: 0.18917837738990784\nEpoch 45, Loss: 0.18910175561904907\nEpoch 46, Loss: 0.18900123238563538\nEpoch 47, Loss: 0.1889697164297104\nEpoch 48, Loss: 0.18893258273601532\nEpoch 49, Loss: 0.18888649344444275\nEpoch 50, Loss: 0.188868448138237\nTraining time: 2.8735427856445312 seconds\n","output_type":"stream"}]}]}